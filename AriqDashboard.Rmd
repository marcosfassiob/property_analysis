---
title: "Property Sale Value Analysis"
author: "Marcos Fassio Bazzi, Ariq Rashid"
output: 
  flexdashboard::flex_dashboard:
    orientation: columns
    vertical_layout: fill
    source_code: embed
    theme: yeti
---

```{r setup, include=FALSE}
library(flexdashboard)
library(knitr)
library(olsrr)
library(leaps)
library(faraway)
library(GGally)
library(tidyverse)
library(glmnet)
library(ISLR2)
library(splines)
library(caret)
library(FNN)
library(e1071)
library(gmodels)
library(car)
library(arm)
library(boot)
```

```{r}
##CHANGE THIS WORKING DIRECTORY--------------------
setwd("C:/Users/azras/OneDrive/Documents/School/VT/Fall 2023/Intermediate Data and ML - CMDA 4654/Project 1")
```

```{r}
df <- read.csv("data/property.csv", header=TRUE)
wake_cols <- c(10, 18:21, 24:29, 34, 43:46, 48, 50, 52, 53, 59, 61) # columns i want in cleaned data
useless_APA_cols <- c("Site In Natural State", "Developing Site") # rows i DONT want in cleaned data
useless_land_class_cols <- c("Part Exempt", "State Assessed", "Vacant Land", "Manufactured Home", 
                       "Manufactured Home Park")
useless_ownership_cols <- c("County, Parish, Province, Etc.", "Federal Government")

df_w <- df %>%
    dplyr::select(all_of(wake_cols)) %>%
    mutate_if(is.character, list(~na_if(.,""))) %>%
    filter(Land.Value != 0 & Building.Value != 0 & Land.Sale.Value > 0 & Total.Sale.Value > 0) %>%
    filter(!APA.Site.Description %in% useless_APA_cols) %>% 
    filter(!Land.Class %in% useless_land_class_cols) %>% 
    filter(!APA.Ownership.Description %in% useless_ownership_cols) %>%
    # only one instance found - essentially an exception
    drop_na()
```

Natural Cubic Splines
=======================================================================

summary {.sidebar}
-----------------------------------------------------------------------

### How Land Value affects Total Sale Value

One of the columns in this data is "Land.Value". It is safe to assume that this has a positive affect on the Total Sale Value of the property, but is it directly proportional? Or is there some some unexpected relationship? To investigate this, I used Natural Cubic Splines to fit a proper line to the data.

Column {.tabset}
-----------------------------------------------------------------

### Different Degrees

First, I filtered the data so it only includes Land Value and Sale Values in a certain range so that outliers would not affect the model. I also sampled it down to 3000 points in order to legibly visualize the data to a plot. Then, I wanted to fit models with different degrees of freedom (1-5) in order to beginning finding the best one. Below is a summary of the 5 models.

```{r}
#Creating dataframe for plot

ns_filtered <- df_w %>%
  filter(Land.Value < 250000, Total.Sale.Value < 900000) %>%
  dplyr::select(Land.Value, Total.Sale.Value)

ns_df <- ns_filtered %>% sample_n(3000)

#Creating NS with different dfs
ns.1 <- lm(Total.Sale.Value ~ ns(Land.Value, df= 1), data = ns_df)
ns.2 <- lm(Total.Sale.Value ~ ns(Land.Value, df= 2), data = ns_df)
ns.3 <- lm(Total.Sale.Value ~ ns(Land.Value, df= 3), data = ns_df)
ns.4 <- lm(Total.Sale.Value ~ ns(Land.Value, df= 4), data = ns_df)
ns.5 <- lm(Total.Sale.Value ~ ns(Land.Value, df= 5), data = ns_df)


summary(ns.1)
summary(ns.2)
summary(ns.3)
summary(ns.4)
summary(ns.5)

```

To determine which degree of freedom is the best, the SSE for each model must be calculated. The formula for this is the sum of squared residuals for each model. 
```{r}
#Determining which df is best
SSE.1 <- sum(residuals(ns.1)^2)
SSE.2 <- sum(residuals(ns.2)^2)
SSE.3 <- sum(residuals(ns.3)^2)
SSE.4 <- sum(residuals(ns.4)^2)
SSE.5 <- sum(residuals(ns.5)^2)
SSE.1 <- c(SSE.1, SSE.2, SSE.3, SSE.4, SSE.5)

sse_data <- data.frame("df" = c(1,2,3,4,5), "SSE" = SSE.1)
kable(sse_data, caption = "df and SSE for NS models")

```

### Best Model

From the SSE table and the Elbow plot, the best model appears to be of degree 5. Below is a summary of the Model.

```{r}
summary(ns.5)
```



Column {.tabset}
------------------------------------------

### Plots for Models with Different Degrees

```{r}
ggplot(data = ns_df, aes(x = Land.Value, y = Total.Sale.Value)) +
  geom_point(size = 0.7, color = "royalblue4") +
  geom_smooth(method = "lm", formula = y ~ ns(x, df = 1), se = FALSE, aes(color = "1st")) +
  geom_smooth(method = "lm", formula = y ~ ns(x, df = 2), se = FALSE, aes(color = "2nd")) +
  geom_smooth(method = "lm", formula = y ~ ns(x, df = 3), se = FALSE, aes(color = "3rd")) +
  geom_smooth(method = "lm", formula = y ~ ns(x, df = 4), se = FALSE, aes(color = "4th")) +
  geom_smooth(method = "lm", formula = y ~ ns(x, df = 5), se = FALSE, aes(color = "5th")) +
  scale_color_manual(
    name = "Polynomial Degree",
    breaks = c("1st", "2nd", "3rd", "4th", "5th"),
    labels = c("1st", "2nd", "3rd", "4th", "5th"),
    values = c("yellow3", "hotpink3", "brown4", "springgreen4", "orange3")
  ) +
  theme(legend.position = "bottom") +
  labs(title = "Natural Cubc Splines", x = "Land Value ($)",
       y = "Totl Sale Value ($)")
```

### Elbow Plot

```{r}
ggplot(data = data.frame(df =c(1,2,3,4,5), SSE.1),       
          aes(x=df, y = SSE.1)) + geom_point() + 
          labs(x="df", y="SSE") + ggtitle("df vs SSE for NS") +   geom_line()
```


### Best Model Plotted

```{r}
ggplot(data = ns_df, aes(x = Land.Value, y = Total.Sale.Value)) +
  geom_point(size = 0.9, color = "royalblue") +
  geom_smooth(method = "lm", formula = y ~ ns(x, df = 5), se = TRUE, 
              color = "brown4", size=2) +
  geom_vline(xintercept = attributes(ns(ns_df$Land.Value, df = 3))$knots, 
             linetype = "dashed", color = "black", size = 1) +
  ggtitle("Natural Cubic Spline on Land Value vs Total Sale Value") +
  labs(x = "Land Value ($)", y = "Total Sale Value ($)") +
  theme_minimal()
```


Ridge Regression
=================================================

summary {.sidebar}
---------------------------------------------------
### Applying Ridge Regression

The point of this method is to address multicolinearity in a model. With this dataset, a lot of the columns that can be helpful may be correlated with each other. When selecting the best Multilinear Regresssion Model, the log of the x values or column variables had to be taken. I want to use this method to see if using the regular values, not taking the log, will produce a possibly better model.

Column {.tabset}
-----------------------------------------------------

### Creating first model

The first step is to create a model using the glmnet library that does the default ridge regression, without setting a lambda. Below is a summary of the base model. In the plot tabs, you can see the plot of the coefficients for this model.

```{r}
mlr_cols <- c(1, 4:8, 10, 18)
df_mlr <- df_w %>%
    dplyr::select(all_of(mlr_cols))
```

```{r}
y <- df_mlr$Total.Sale.Value
selected_columns <- c('Calculated.Acreage', 'Total.Structures', 'Land.Sale.Value')

# Select the columns from the data frame
selected_data <- df_mlr[, selected_columns]

# Convert the selected data to a matrix
x <- as.matrix(selected_data)

ridge_model1 <- glmnet(x, y, alpha = 0)
kable(summary(ridge_model1))
```
### Finding the Best Lambda

In order to find the best lambda, we must do cross validation. The glmnet library has a built in function for that. This process is visualized in the Cross-Validated Model tab. To see if using this improved the previous model, we calculate the R-squared value to see how much fo the variation is accounted for. The coeffiecients plot with best lambda is in the "Best Model" tab.

```{r}
cv_model1 <- cv.glmnet(x, y, alpha = 0)
best_lambda1 <- cv_model1$lambda.min
best_ridge_model <- glmnet(x, y, alpha = 0, lambda = best_lambda1)


#use fitted best model to make predictions
y_predicted <- predict(ridge_model1, s = best_lambda1, newx = x)

#find SST and SSE
sst <- sum((y - mean(y))^2)
sse <- sum((y_predicted - y)^2)

#find R-Squared
rsq <- 1 - sse/sst

values <- c(best_lambda1, rsq)


```

Below is the best lambda value.
```{r}
best_lambda1
```

Below are the coefficients.
```{r}
coef(best_ridge_model)
```

Below is the Summary of the best ridge regression Model.
```{r}
kable(summary(best_ridge_model))
```

The whole point of using this method was to see if we could get an improve model that addresses multicolinearity. After calculating the R-squared value, it seems that this model is a slight improvement of the same variables and predicted value from the previous model selection.
```{r}
rsq
```

Column {.tabset}
-----------------------------------------------

### First Model Coeffecients Plot

````{r}
plot(ridge_model1)
```

### Plotting Cross-Validated Model

```{r}
plot(cv_model1)
```

### Plotting Best Model

```{r}
plot(ridge_model1, xvar = "lambda")
```
