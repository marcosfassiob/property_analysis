---
title: "AriqWork"
author: "Ariq Rashid"
date: "2023-11-06"
output: 
  pdf_document:
    highlight: haddock
keep_tex: no
number_sections: no
html_document:
  df_print: paged
geometry: margin = 0.5in
header-includes:
- \usepackage{booktabs}
- \usepackage{longtable}
- \usepackage{array}
- \usepackage{multirow}
- \usepackage{wrapfig}
- \usepackage{float}
- \usepackage{colortbl}
- \usepackage{pdflscape}
- \usepackage{tabu}
- \usepackage{threeparttable}
- \usepackage{threeparttablex}
- \usepackage[normalem]{ulem}
- \usepackage{makecell}
- \usepackage{xcolor}
editor_options:
  chunk_output_type: console
documentclass: article
urlcolor: blue
---

```{r setup, include=FALSE}
library(knitr) # I recommend doing this here
library(olsrr)
library(leaps)
library(ggplot2)
library(faraway)
library(GGally)
library(tidyverse)
library(glmnet)
library(ISLR2)
library(splines)
library(caret)
library(FNN)
library(fANCOVA)
library(dplyr)


# Although you can call functions from a library using the following notation
#  without loading the entire library.
knitr::opts_chunk$set(echo = TRUE, 
                      comment = NA, # Required
                      fig.path = "./figures/",  # Store all figures here in relative path (make the folder first)
                      fig.align = "center",
                      fig.width = 7,
                      fig.height = 7,
                      message = FALSE, # Turn off load messages
                      warning = FALSE # Turn off warnings
                      )
setwd("C:/Users/azras/OneDrive/Documents/School/VT/Fall 2023/Intermediate Data and ML - CMDA 4654/Project 1")
```

# Loading the Cleaned Data
```{r}
data <- data.frame(read.csv("cleaned_data.csv"))
View(data)
```

# Ridge Regression
```{r}
y <- data$Total.Sale.Value
x <- data.matrix(data[, c('Calculated.Acreage', 'Total.Structures', 'Total.Units', 'Building.Value','Land.Value','Land.Sale.Value')])
```

```{r}
ridge_model1 <- glmnet(x, y, alpha = 0)
cv_model1 <- cv.glmnet(x, y, alpha = 0)
best_lambda1 <- cv_model1$lambda.min
best_lambda1
plot(cv_model1)
```

```{r}
best_ridge_model <- glmnet(x, y, alpha = 0, lambda = best_lambda1)
coef(best_ridge_model)
```

```{r}
plot(ridge_model1, xvar = "lambda")
```

```{r}
#use fitted best model to make predictions
y_predicted <- predict(ridge_model1, s = best_lambda1, newx = x)

#find SST and SSE
sst <- sum((y - mean(y))^2)
sse <- sum((y_predicted - y)^2)

#find R-Squared
rsq <- 1 - sse/sst
rsq
```

---------------------------------------------------------------------------
# kNN Classification
```{r}
desired_columns <- c('Billing.Class','Total.Sale.Value','Calculated.Acreage',
                     'Total.Structures', 'Total.Units',
                     'Building.Value','Land.Value','Land.Sale.Value')
df <- data[, desired_columns]
View(df)
```

```{r}
index <- sample(1:nrow(df), round(nrow(df) * 0.7))
training_df <- df[index, ]
testing_df <- df[-index, ]

# Store the training/testing data features
train_features1 <- training_df[, 2:8]
test_features1 <- testing_df[, 2:8]

# Scale the features
train_features <- scale(train_features1)
test_features <- scale(test_features1)

# Store the actual labels (assuming Total.Sale.Value is a factor)
train_classes <- factor(training_df$Billing.Class)
test_classes <- factor(testing_df$Billing.Class)
```

```{r}
knn_classes <- knn(train = train_features, test = test_features,
cl = train_classes, k = 5)
knn_classes[1:nrow(test_features)]
```

```{r}

confusionMatrix(data = knn_classes, reference = test_classes)
#print(confusion_matrix)
```

## Gonna try binning them into years

```{r}
years_binned <- data
years_binned$Time.Period <- ifelse(data$Year < 1995, "Old",
                            ifelse(data$Year >= 1995 & data$Year <= 2018,
                                   "Modern","New"))
```

```{r}
desired_columns <- c('Time.Period','Total.Sale.Value',
                     'Total.Structures', 'Total.Units',
                     'Building.Value','Land.Value','Land.Sale.Value')
knn_df <- years_binned[, desired_columns]
```

```{r}
index <- sample(1:nrow(knn_df), round(nrow(knn_df) * 0.7))
training_df <- knn_df[index, ]
testing_df <- knn_df[-index, ]

# Store the training/testing data features
train_features1 <- training_df[, 2:7]
test_features1 <- testing_df[, 2:7]

# Scale the features
train_features <- scale(train_features1)
test_features <- scale(test_features1)

# Store the actual labels (assuming Total.Sale.Value is a factor)
train_classes <- factor(training_df$Time.Period)
test_classes <- factor(testing_df$Time.Period)
```

```{r}
knn_classes <- knn(train = train_features, test = test_features,
cl = train_classes, k = 5)
```

```{r}
confusionMatrix(data = knn_classes, reference = test_classes)
```

Only able to get 70% now

-------------------------------------------------------------
# NATURAL CUBIC SPLINES

```{r}
ns_filtered_df <- df[df$Land.Value < 250000 & df$Total.Sale.Value < 900000,]
set.seed(123)  # Set a seed for reproducibility
ns_less_df <- ns_filtered_df %>% sample_n(3000)  # Adjust the number of sampled points as needed

```

```{r}
ns_filtered <- data %>%
  filter(Land.Value < 250000, Total.Sale.Value < 900000) %>%
  select(Land.Value, Total.Sale.Value)

ns_df <- ns_filtered %>% sample_n(3000)
```

```{r}
ns.1 <- lm(Total.Sale.Value ~ ns(Land.Value, df= 1), 
                      data = ns_df)
ns.2 <- lm(Total.Sale.Value ~ ns(Land.Value, df= 2), 
                      data = ns_df)
ns.3 <- lm(Total.Sale.Value ~ ns(Land.Value, df= 3), 
                      data = ns_df)
ns.4 <- lm(Total.Sale.Value ~ ns(Land.Value, df= 4), 
                      data = ns_df)
ns.5 <- lm(Total.Sale.Value ~ ns(Land.Value, df= 5), 
                      data = ns_df)

```

```{r}
SSE.1 <- sum(residuals(ns.1)^2)
SSE.2 <- sum(residuals(ns.2)^2)
SSE.3 <- sum(residuals(ns.3)^2)
SSE.4 <- sum(residuals(ns.4)^2)
SSE.5 <- sum(residuals(ns.5)^2)

SSE.1 <- c(SSE.1, SSE.2, SSE.3, SSE.4, SSE.5)
```

```{r}
sse_data <- data.frame("df" = c(1,2,3,4,5), "SSE" = SSE.1)
kable(sse_data, caption = "df and SSE for NS models")
```

```{r}
ggplot(data = data.frame(df =c(1,2,3,4,5), SSE.1),       
          aes(x=df, y = SSE.1)) + geom_point() + 
          labs(x="df", y="SSE") + ggtitle("df vs SSE for NS") +   geom_line()
```

3 is best?

```{r}
ggplot(data = ns_df, aes(x = Land.Value, y = Total.Sale.Value)) +
  geom_point(size = 0.9, color = "royalblue") +
  geom_smooth(method = "lm", formula = y ~ ns(x, df = 3), se = TRUE, 
              color = "brown4", size=2) +
  geom_vline(xintercept = attributes(ns(ns_df$Land.Value, df = 3))$knots, 
             linetype = "dashed", color = "black", size = 1) +
  ggtitle("Natural Cubic Spline on Land Value vs Total Sale Value") +
  labs(x = "Land Value ($)", y = "Total Sale Value ($)") +
  theme_minimal() +
  theme(plot.title = element_text(size = 18, hjust = 0.5),
        axis.title = element_text(size = 14),
        axis.text = element_text(size = 12),
        legend.position = "none")
```